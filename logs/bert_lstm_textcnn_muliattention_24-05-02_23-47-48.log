> creating model bert
> training arguments:
>>> num_classes: 2
>>> model_name: bert
>>> method_name: lstm_textcnn_muliattention
>>> train_batch_size: 32
>>> test_batch_size: 32
>>> num_epoch: 10
>>> lr: 1e-05
>>> weight_decay: 0.01
>>> device: cpu
>>> backend: False
>>> workers: 0
>>> timestamp: 1714664868962
>>> log_name: bert_lstm_textcnn_muliattention_24-05-02_23-47-48.log
1/10 - 10.00%
[train] loss: 0.6856, acc: 56.03, precision: 0.7093, recall: 0.5603, f1: 0.4616
[test] loss: 0.6505, acc: 79.78, precision: 0.8052, recall: 0.7978, f1: 0.7942
2/10 - 20.00%
[train] loss: 0.5687, acc: 85.89, precision: 0.8607, recall: 0.8589, f1: 0.8588
[test] loss: 0.5057, acc: 84.15, precision: 0.8435, recall: 0.8415, f1: 0.8404
3/10 - 30.00%
[train] loss: 0.4326, acc: 90.96, precision: 0.9098, recall: 0.9096, f1: 0.9096
[test] loss: 0.4963, acc: 81.97, precision: 0.8372, recall: 0.8197, f1: 0.8195
4/10 - 40.00%
[train] loss: 0.4033, acc: 91.51, precision: 0.9153, recall: 0.9151, f1: 0.9151
[test] loss: 0.4767, acc: 83.61, precision: 0.8365, recall: 0.8361, f1: 0.8354
5/10 - 50.00%
[train] loss: 0.3675, acc: 95.07, precision: 0.9507, recall: 0.9507, f1: 0.9507
[test] loss: 0.4521, acc: 85.79, precision: 0.8596, recall: 0.8579, f1: 0.8571
6/10 - 60.00%
[train] loss: 0.3730, acc: 94.11, precision: 0.9413, recall: 0.9411, f1: 0.9411
[test] loss: 0.5061, acc: 80.33, precision: 0.8098, recall: 0.8033, f1: 0.8001
7/10 - 70.00%
[train] loss: 0.3799, acc: 93.29, precision: 0.9331, recall: 0.9329, f1: 0.9329
[test] loss: 0.4592, acc: 85.25, precision: 0.8533, recall: 0.8525, f1: 0.8526
8/10 - 80.00%
[train] loss: 0.3620, acc: 95.21, precision: 0.9522, recall: 0.9521, f1: 0.9521
[test] loss: 0.4536, acc: 85.79, precision: 0.8578, recall: 0.8579, f1: 0.8578
9/10 - 90.00%
[train] loss: 0.3585, acc: 95.62, precision: 0.9564, recall: 0.9562, f1: 0.9562
[test] loss: 0.4576, acc: 85.79, precision: 0.8579, recall: 0.8579, f1: 0.8579
10/10 - 100.00%
[train] loss: 0.3539, acc: 95.89, precision: 0.9590, recall: 0.9589, f1: 0.9589
[test] loss: 0.4526, acc: 85.79, precision: 0.8603, recall: 0.8579, f1: 0.8582
Training time: 10600.91 seconds
best loss: 0.4521, best acc: 85.79
log saved: bert_lstm_textcnn_muliattention_24-05-02_23-47-48.log
