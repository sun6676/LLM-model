> creating model roberta
> training arguments:
>>> num_classes: 2
>>> model_name: roberta
>>> method_name: lstm_textcnn_attention
>>> train_batch_size: 32
>>> test_batch_size: 32
>>> num_epoch: 10
>>> lr: 1e-05
>>> weight_decay: 0.01
>>> device: cpu
>>> backend: False
>>> workers: 0
>>> timestamp: 1716798765113
>>> checkpoint_dir: ./checkpoints
>>> log_name: roberta_lstm_textcnn_attention_24-05-27_16-32-45.log
尝试从检查点继续训练: ./checkpoints\best_model.pth
没有找到检查点文件，将从头开始训练。
新最佳模型已保存于: ./checkpoints\best_model_epoch1_acc87.47_loss0.4394.pth
1/10 - 10.00%
[train] loss: 0.5741, acc: 72.87, precision: 0.7352, recall: 0.7287, f1: 0.7265
[test] loss: 0.4394, acc: 87.47, precision: 0.8778, recall: 0.8747, f1: 0.8743
新最佳模型已保存于: ./checkpoints\best_model_epoch2_acc88.51_loss0.4237.pth
2/10 - 20.00%
[train] loss: 0.4236, acc: 88.78, precision: 0.8882, recall: 0.8878, f1: 0.8878
[test] loss: 0.4237, acc: 88.51, precision: 0.8851, recall: 0.8851, f1: 0.8851
新最佳模型已保存于: ./checkpoints\best_model_epoch3_acc90.86_loss0.4045.pth
3/10 - 30.00%
[train] loss: 0.3968, acc: 91.60, precision: 0.9161, recall: 0.9160, f1: 0.9160
[test] loss: 0.4045, acc: 90.86, precision: 0.9087, recall: 0.9086, f1: 0.9086
4/10 - 40.00%
[train] loss: 0.3837, acc: 92.80, precision: 0.9282, recall: 0.9280, f1: 0.9280
[test] loss: 0.4035, acc: 90.65, precision: 0.9068, recall: 0.9065, f1: 0.9065
新最佳模型已保存于: ./checkpoints\best_model_epoch5_acc91.14_loss0.3983.pth
5/10 - 50.00%
[train] loss: 0.3788, acc: 93.30, precision: 0.9331, recall: 0.9330, f1: 0.9330
[test] loss: 0.3983, acc: 91.14, precision: 0.9114, recall: 0.9114, f1: 0.9114
新最佳模型已保存于: ./checkpoints\best_model_epoch6_acc91.19_loss0.3984.pth
6/10 - 60.00%
[train] loss: 0.3693, acc: 94.29, precision: 0.9431, recall: 0.9429, f1: 0.9429
[test] loss: 0.3984, acc: 91.19, precision: 0.9128, recall: 0.9119, f1: 0.9119
新最佳模型已保存于: ./checkpoints\best_model_epoch7_acc91.79_loss0.3944.pth
7/10 - 70.00%
[train] loss: 0.3664, acc: 94.65, precision: 0.9466, recall: 0.9465, f1: 0.9465
[test] loss: 0.3944, acc: 91.79, precision: 0.9186, recall: 0.9179, f1: 0.9179
8/10 - 80.00%
[train] loss: 0.3638, acc: 94.90, precision: 0.9490, recall: 0.9490, f1: 0.9490
[test] loss: 0.3976, acc: 91.41, precision: 0.9147, recall: 0.9141, f1: 0.9141
新最佳模型已保存于: ./checkpoints\best_model_epoch9_acc92.07_loss0.3902.pth
9/10 - 90.00%
[train] loss: 0.3571, acc: 95.58, precision: 0.9558, recall: 0.9558, f1: 0.9558
[test] loss: 0.3902, acc: 92.07, precision: 0.9207, recall: 0.9207, f1: 0.9207
新最佳模型已保存于: ./checkpoints\best_model_epoch10_acc92.34_loss0.3886.pth
10/10 - 100.00%
[train] loss: 0.3536, acc: 95.95, precision: 0.9595, recall: 0.9595, f1: 0.9595
[test] loss: 0.3886, acc: 92.34, precision: 0.9234, recall: 0.9234, f1: 0.9234
Training time: 99352.78 seconds
best loss: 0.3886, best acc: 92.34
log saved: roberta_lstm_textcnn_attention_24-05-27_16-32-45.log
