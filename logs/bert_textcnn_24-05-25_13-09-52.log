> creating model bert
> training arguments:
>>> num_classes: 2
>>> model_name: bert
>>> method_name: textcnn
>>> train_batch_size: 64
>>> test_batch_size: 64
>>> num_epoch: 4
>>> lr: 2e-05
>>> weight_decay: 0.01
>>> device: cpu
>>> backend: False
>>> workers: 0
>>> timestamp: 1716613792530
>>> checkpoint_dir: ./checkpoints
>>> log_name: bert_textcnn_24-05-25_13-09-52.log
尝试从检查点继续训练: ./checkpoints\best_model.pth
没有找到检查点文件，将从头开始训练。
新最佳模型已保存于: ./checkpoints\best_model_epoch1_acc98.30_loss0.3305.pth
1/4 - 25.00%
[train] loss: 0.3959, acc: 93.64, precision: 0.9415, recall: 0.9364, f1: 0.9362
[test] loss: 0.3305, acc: 98.30, precision: 0.9836, recall: 0.9830, f1: 0.9830
新最佳模型已保存于: ./checkpoints\best_model_epoch2_acc98.32_loss0.3301.pth
2/4 - 50.00%
[train] loss: 0.3745, acc: 95.28, precision: 0.9568, recall: 0.9528, f1: 0.9527
[test] loss: 0.3301, acc: 98.32, precision: 0.9838, recall: 0.9832, f1: 0.9832
3/4 - 75.00%
[train] loss: 0.3748, acc: 95.15, precision: 0.9557, recall: 0.9515, f1: 0.9514
[test] loss: 0.3368, acc: 97.65, precision: 0.9775, recall: 0.9765, f1: 0.9765
